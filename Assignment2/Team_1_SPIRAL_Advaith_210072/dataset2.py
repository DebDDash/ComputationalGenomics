# -*- coding: utf-8 -*-
"""Dataset2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1P3mkLd7fpSCE7f9tvY_RthnYdpwixf3-

# Pre Processing
"""

pip install scanpy

!pip install scikit-misc

import pandas as pd
import numpy as np
import scanpy as sc
import matplotlib.pyplot as plt
import os
import anndata
import scipy as sp
import umap.umap_ as umap
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA

"""MOSTA"""

sample=['2']
Batch_list = []
for i in np.arange(len(sample)):
    adata=sc.read_h5ad("/content/Dataset2_osmFISH.h5ad") # Reads File
    print(adata)
    adata.obs_names = [x + '_' + sample[i] for x in adata.obs_names] # Changes the index names
    sc.pp.normalize_total(adata, target_sum=1e4) # Normalizing
    sc.pp.log1p(adata) # log transformation
    sc.pp.highly_variable_genes(adata, flavor="seurat_v3", n_top_genes=5000) # finds highly variable genes
    adata = adata[:, adata.var['highly_variable']] # Retains only highly variable
    Batch_list.append(adata) # Adds adata to batch list

Batch_list

adata

adata.obs_names

adata_concat = anndata.concat(Batch_list, label="slice_name", keys=sample)
vf=np.array(adata_concat.var.index)
for i in np.arange(len(sample)):
    adata1=adata_concat[adata_concat.obs['slice_name']==sample[i],:].copy()
    cells=adata1.obs_names
    # Convert adata1.X to a sparse matrix if it's not already
    if not isinstance(adata1.X, (sp.sparse.csr_matrix, sp.sparse.csc_matrix)):
        adata1.X = sp.sparse.csr_matrix(adata1.X)
    mat=pd.DataFrame(adata1.X.todense(),index=cells,columns=vf)
    coord=pd.DataFrame(adata1.obsm['spatial'],index=cells,columns=['x','y'])
    adata1.obs['annotation']= adata.obs_names
    meta=adata1.obs[['annotation','slice_name']]
    meta.columns=['celltype','batch']
    meta.index=cells
    mat.to_csv("/"+sample[i]+"_mat.csv") # creates mat.csv
    meta.to_csv("/"+sample[i]+"_meta.csv") # creates meta.csv
    coord.to_csv("/"+sample[i]+"_coord.csv") # creates coord.csv

"""dirs="/data02/tguo/space_batch_effect/human_DLPFC_10x/"
sample_name=[151507,151508,151509,151510,151669,151670,151671,151672,151673,151674,151675,151676]
IDX=np.arange(0,12)
VF=[]
MAT=[]
flags=str(sample_name[IDX[0]])
for i in np.arange(1,len(IDX)):
    flags=flags+'-'+str(sample_name[IDX[i]])
flags=flags+"_"
for k in np.arange(len(IDX)):
    adata = sc.read_visium(path=dirs+"scanpy_file/"+str(sample_name[IDX[k]])+"/",
                       count_file="filtered_feature_bc_matrix.h5")
    adata.var_names_make_unique()
    Ann_df=pd.read_csv(dirs+"input/"+str(sample_name[IDX[k]])+"_label.txt", sep=',', header=0, index_col=0)
    Ann_df.columns=['Ground Truth','Sample']
    Ann_df.index=[i.split('-')[1]+'-'+i.split('-')[2] for i in Ann_df.index]
    adata=adata[Ann_df.index,:]
    adata.obs['Ground Truth']=Ann_df.loc[:,'Ground Truth']
    sc.pp.highly_variable_genes(adata, flavor="seurat_v3", n_top_genes=1000)
    sc.pp.normalize_total(adata, target_sum=1e4)
    sc.pp.log1p(adata)
    adata.obs['batch']=str(sample_name[IDX[k]])
    cells=[str(sample_name[IDX[k]])+'-'+i for i in adata.obs_names]
    mat1=pd.DataFrame(adata.X.toarray(),columns=adata.var_names,index=cells)
    coord1=pd.DataFrame(adata.obsm['spatial'],columns=['x','y'],index=cells)
    meta1=adata.obs[['Ground Truth', 'batch']]
    meta1.columns=['celltype','batch']
    meta1.index=cells
    meta1.to_csv(dirs+"gtt_input_scanpy/"+flags+str(sample_name[IDX[k]])+"_label-1.txt")
    coord1.to_csv(dirs+"gtt_input_scanpy/"+flags+str(sample_name[IDX[k]])+"_positions-1.txt")
    MAT.append(mat1)
    VF=np.union1d(VF,adata.var_names[adata.var['highly_variable']])

for i in np.arange(len(IDX)):
    mat=MAT[i]
    mat=mat.loc[:,VF]
    mat.to_csv(dirs+"gtt_input_scanpy/"+flags+str(sample_name[IDX[i]])+"_features-1.txt")

# Generating KNN Graph
"""

import sklearn.neighbors
import scipy.sparse as sp

def Cal_Spatial_Net(adata, rad_cutoff=None, k_cutoff=None, model='Radius', verbose=True):
    """\
    Construct the spatial neighbor networks.

    Parameters
    ----------
    adata
        AnnData object of scanpy package.
    rad_cutoff
        radius cutoff when model='Radius'
    k_cutoff
        The number of nearest neighbors when model='KNN'
    model
        The network construction model. When model=='Radius', the spot is connected to spots whose distance is less than rad_cutoff. When model=='KNN', the spot is connected to its first k_cutoff nearest neighbors.

    Returns
    -------
    The spatial networks are saved in adata.uns['Spatial_Net']
    """

    assert(model in ['Radius', 'KNN'])
    if verbose:
        print('------Calculating spatial graph...')
    coor = pd.DataFrame(adata.obsm['spatial'])
    coor.index = adata.obs.index
#     coor.columns = ['imagerow', 'imagecol']

    if model == 'Radius':
        nbrs = sklearn.neighbors.NearestNeighbors(radius=rad_cutoff).fit(coor)
        distances, indices = nbrs.radius_neighbors(coor, return_distance=True)
        KNN_list = []
        for it in range(indices.shape[0]):
            KNN_list.append(pd.DataFrame(zip([it]*indices[it].shape[0], indices[it], distances[it])))

    if model == 'KNN':
        nbrs = sklearn.neighbors.NearestNeighbors(n_neighbors=k_cutoff+1).fit(coor)
        distances, indices = nbrs.kneighbors(coor)
        KNN_list = []
        for it in range(indices.shape[0]):
            KNN_list.append(pd.DataFrame(zip([it]*indices.shape[1],indices[it,:], distances[it,:])))

    KNN_df = pd.concat(KNN_list)
    KNN_df.columns = ['Cell1', 'Cell2', 'Distance']

    Spatial_Net = KNN_df.copy()
    Spatial_Net = Spatial_Net.loc[Spatial_Net['Distance']>0,]
    id_cell_trans = dict(zip(range(coor.shape[0]), np.array(coor.index), ))
    Spatial_Net['Cell1'] = Spatial_Net['Cell1'].map(id_cell_trans)
    Spatial_Net['Cell2'] = Spatial_Net['Cell2'].map(id_cell_trans)
    if verbose:
        print('The graph contains %d edges, %d cells.' %(Spatial_Net.shape[0], adata.n_obs))
        print('%.4f neighbors per cell on average.' %(Spatial_Net.shape[0]/adata.n_obs))

    adata.uns['Spatial_Net'] = Spatial_Net

knn=8
rad=1.5
mode='KNN'
mode_num=knn
dirs="/"
sample="SlideV2"
feat=pd.read_csv(dirs+"2_mat.csv",header=0,index_col=0,sep=',') # reads mat.csv
coord=pd.read_csv(dirs+"2_coord.csv",header=0,index_col=0,sep=',') # reads coord.csv
coord.columns=['x','y']
adata = sc.AnnData(feat) # creates an adata using mat
adata.var_names_make_unique() # making var names unique
adata.X=sp.csr_matrix(adata.X) # compressed sparse row matrix
adata.obsm["spatial"] = coord.loc[:,['x','y']].to_numpy() # adding the "spatial" key
Cal_Spatial_Net(adata, rad_cutoff=rad, k_cutoff=knn, model=mode, verbose=True) # calculating KNN
if 'highly_variable' in adata.var.columns:
    adata_Vars =  adata[:, adata.var['highly_variable']]
else:
    adata_Vars = adata
features = pd.DataFrame(adata_Vars.X.toarray()[:, ], index=adata_Vars.obs.index, columns=adata_Vars.var.index)
cells = np.array(features.index)
cells_id_tran = dict(zip(cells, range(cells.shape[0])))
if 'Spatial_Net' not in adata.uns.keys():
    raise ValueError("Spatial_Net is not existed! Run Cal_Spatial_Net first!")

Spatial_Net = adata.uns['Spatial_Net']
G_df = Spatial_Net.copy()


a=G_df.values[:,:2]
np.savetxt(dirs+"2_edge_"+mode+"_"+str(mode_num)+".csv",G_df.values[:,:2],fmt='%s')

"""# Running SPIRAL"""

import torch
if torch.cuda.is_available():
    device = torch.device("cuda")
    print("Using CUDA")
else:
    device = torch.device("cpu")
    print("Using CPU")

!git clone https://github.com/guott15/SPIRAL.git

# Commented out IPython magic to ensure Python compatibility.
# %cd SPIRAL

!python setup.py build
!python setup.py install

pip install POT

import os
import numpy as np
import argparse


import pandas as pd
from sklearn.decomposition import PCA
from operator import itemgetter
import random
import matplotlib.pyplot as plt
import umap.umap_ as umap
import time

import torch
from spiral.main import SPIRAL_integration
from spiral.layers import *
from spiral.utils import *
from spiral.CoordAlignment import CoordAlignment
from sklearn.metrics.pairwise import euclidean_distances
R_dirs="/home/tguo/tguo2/miniconda3/envs/stnet/lib/R"
os.environ['CUDA_VISIBLE_DEVICES'] = '0'

dirs="/"
samples=["2"]
# extra_dirs="BGI_SlideV2_10X/"
# samples=['Stereo_bin34','SlideV2']
# extra_dirs="Stereo-bin34-filter/"
SEP=','
net_cate='_KNN_'
knn=8

N_WALKS=knn
WALK_LEN=1
N_WALK_LEN=knn
NUM_NEG=knn


feat_file=[]
edge_file=[]
meta_file=[]
coord_file=[]
flags=''
for i in range(len(samples)):
    feat_file.append(dirs+str(samples[i])+"_mat.csv")
    edge_file.append(dirs+str(samples[i])+"_edge"+net_cate+str(knn)+".csv")
    meta_file.append(dirs+str(samples[i])+"_meta.csv")
    coord_file.append(dirs+str(samples[i])+"_coord.csv")
    flags=flags+'_'+samples[i]
N=pd.read_csv(feat_file[0],header=0,index_col=0).shape[1]
if (len(samples)==2):
    M=1
else:
    M=len(samples)

parser = argparse.ArgumentParser()

parser.add_argument('--seed', type=int, default=0, help='The seed of initialization.')
parser.add_argument('--AEdims', type=list, default=[N,[512],32], help='Dim of encoder.')
parser.add_argument('--AEdimsR', type=list, default=[32,[512],N], help='Dim of decoder.')
parser.add_argument('--GSdims', type=list, default=[512,32], help='Dim of GraphSAGE.')
parser.add_argument('--zdim', type=int, default=32, help='Dim of embedding.')
parser.add_argument('--znoise_dim', type=int, default=4, help='Dim of noise embedding.')
parser.add_argument('--CLdims', type=list, default=[4,[],M], help='Dim of classifier.')
parser.add_argument('--DIdims', type=list, default=[28,[32,16],M], help='Dim of discriminator.')
parser.add_argument('--beta', type=float, default=1.0, help='weight of GraphSAGE.')
parser.add_argument('--agg_class', type=str, default=MeanAggregator, help='Function of aggregator.')
parser.add_argument('--num_samples', type=int, default=20, help='number of neighbors to sample.')

parser.add_argument('--N_WALKS', type=int, default=N_WALKS, help='number of walks of random work for postive pairs.')
parser.add_argument('--WALK_LEN', type=int, default=WALK_LEN, help='walk length of random work for postive pairs.')
parser.add_argument('--N_WALK_LEN', type=int, default=N_WALK_LEN, help='number of walks of random work for negative pairs.')
parser.add_argument('--NUM_NEG', type=int, default=NUM_NEG, help='number of negative pairs.')

parser.add_argument('--epochs', type=int, default=100, help='Number of epochs to train.')
parser.add_argument('--batch_size', type=int, default=512, help='Size of batches to train.')
parser.add_argument('--lr', type=float, default=1e-3, help='Initial learning rate.')
parser.add_argument('--weight_decay', type=float, default=5e-4, help='Weight decay.')
parser.add_argument('--alpha1', type=float, default=N, help='Weight of decoder loss.')
parser.add_argument('--alpha2', type=float, default=1, help='Weight of GraphSAGE loss.')
parser.add_argument('--alpha3', type=float, default=1, help='Weight of classifier loss.')
parser.add_argument('--alpha4', type=float, default=1, help='Weight of discriminator loss.')
parser.add_argument('--lamda', type=float, default=1, help='Weight of GRL.') #####Stereo-seq:35um resolution
# parser.add_argument('--lamda', type=float, default=0.1, help='Weight of GRL.')######Stereo-seq: 25um resolution
parser.add_argument('--Q', type=float, default=10, help='Weight negative loss for sage losss.')

params,unknown=parser.parse_known_args()

SPII=SPIRAL_integration(params,feat_file,edge_file,meta_file)

SPII.train()
if not os.path.exists(dirs+"model/"):
    os.makedirs(dirs+"model/")

SPII.model.eval()
all_idx=np.arange(SPII.feat.shape[0])
all_layer,all_mapping=layer_map(all_idx.tolist(),SPII.adj,len(SPII.params.GSdims))
all_rows=SPII.adj.tolil().rows[all_layer[0]]
all_feature=torch.Tensor(SPII.feat.iloc[all_layer[0],:].values).float().cuda()
all_embed,ae_out,clas_out,disc_out=SPII.model(all_feature,all_layer,all_mapping,all_rows,SPII.params.lamda,SPII.de_act,SPII.cl_act)
[ae_embed,gs_embed,embed]=all_embed
[x_bar,x]=ae_out
embed=embed.cpu().detach()
embed=embed[:,SPII.params.znoise_dim:]
names=['GTT_'+str(i) for i in range(embed.shape[1])]
embed1=pd.DataFrame(np.array(embed),index=SPII.feat.index,columns=names)
if not os.path.exists(dirs+"gtt_output/"):
    os.makedirs(dirs+"gtt_output/")

embed_file=dirs+"gtt_output/SPIRAL"+flags+"_embed_"+str(SPII.params.batch_size)+".csv" # generates csv
embed1.to_csv(embed_file)

batch=SPII.meta.loc[:,'batch']
ub=np.unique(batch)
import umap.umap_ as umap
import matplotlib.pyplot as plt
umap_mat=umap.UMAP().fit_transform(embed)
colour2=['r','g','black','cyan','blue','purple','yellow','grey']
colour1=['lightcoral','red','tomato','chocolate','darkorange','gold','olive','yellow','yellowgreen','lawngreen','forestgreen','lime',
       'cyan','deepskyblue','dodgerblue','royalblue','blue','blueviolet','purple','fuchsia','hotpink','pink','gray','black','teal']
f, axs= plt.subplots(1,1,figsize=(20,10))
size=10
for i in range(len(ub)):
    axs.scatter(umap_mat[np.where(batch==ub[i])[0],0],umap_mat[np.where(batch==ub[i])[0],1],c=colour2[i],s=size)

axs.set_xlabel("umap1",fontsize=30)
axs.set_ylabel("umap2",fontsize=30)
axs.legend(ub,loc="best",fontsize=20,markerscale=4,bbox_to_anchor=(-0.19,1))
axs.tick_params(axis='both', which='major', labelsize=20)

"""R file to be run after the steps above."""

import anndata
import scanpy as sc
adata=anndata.AnnData(SPII.feat)
adata.obsm['spiral']=embed1.values
adata.obs['batch']=SPII.meta.loc[:,'batch'].values
cluster_file = '/_2_seuratmethod_clust_modify2.csv'
coord=pd.read_csv(coord_file[0],header=0,index_col=0)
for i in np.arange(1,len(samples)):
    coord=pd.concat((coord,pd.read_csv(coord_file[i],header=0,index_col=0)))

adata.obsm['spatial']=coord.loc[adata.obs_names,:].values
cluster=pd.read_csv(cluster_file,header=0,index_col=0)
adata.obs['SPIRAL']=cluster.loc[adata.obs_names,:]

adata.obs['SPIRAL']=adata.obs['SPIRAL'].astype('category')
size1=100
metric='SPIRAL'
sc.pl.spatial(adata,color=metric, spot_size=size1,legend_loc='on data')